Btrfsは(ext4やXFSなどの)従来型ファイルシステムが提供する機能に加えて、次のような豊富な機能を提供します。

- ストレージプール: 1つないし複数のデバイス(あるいはパーティション。以下単に区別せず、デバイスと記載)から、一つのストレージプールを作成できます
- サブボリューム: サブボリュームとは、上記ストレージプールから切り出したマウント可能な領域です
- スナップショット: ある時点のサブボリュームの内容を持つスナップショットを高速に採取できます
- スナップショットの転送: スナップショットのデータをシリアライズしてファイルや別ホストに転送します。
- RAID: ストレージプールはBtrfs内蔵のRAID機能を用いてデータを保護できます(RAID0,1,10,5,6をサポート)
- データ破壊検知/修復: ストレージからのデータの読み出し時にデータの破壊を検出できます。ストレージプールのRAID構成が、データのコピーやパリティを持つもの(1,10,5,6)である場合、正しいデータを修復できます。修復できない構成であればデータを捨ててエラーを返します。
- 透過的圧縮: データを圧縮した状態でストレージに書き込むことによってストレージの容量、および帯域を節約可能です(CPU使用量は増えます)

上記の機能はすべてアンマウントすることなくオンラインで実施できるため、可用性が高いのがBtrfsの大きな利点です。

本章では、まずBtrfsが提供するほとんどの機能の実現に欠かせない「コピーオンライト型のデータ更新」という考え方について説明します。その後で、それぞれの機能について説明します。

# コピーオンライト型のデータ更新

Btrfsはデータの更新方法がext4やXFSなどの従来型のファイルシステムとは根本的に異なります。ext4やXFSは、あるデータを更新する際に、元と同じ場所に上書きします。ここではこれを上書き型更新と呼びます。上書き型更新においては、複数のデータを更新する必要がある際に、更新中に強制電源断(やカーネルパニックなど)が発生すると、ファイルシステムが不整合な状態になります。

![overwrite](images/overwrite.png)

更新しようとしたデータがユーザデータの場合は更新中のデータが見えてしまいますし、メタデータの場合は、あるはずのないファイルが見えたり、でたらめなファイルが見えたりしていまします。これを避けるために上書き型更新を使うファイルシステムではジャーナリングという方法でデータの整合性を確保する方法がとられることが多いです(ジャーナリングについての説明は割愛します)。すべてのデータのジャーナリングを有効にすると、書き込み量がおよそ倍になってしまうため、ext4やXFSではメタデータのみのジャーナリングを主に使います(XFSにおいてはそもそもユーザデータのジャーナリング機能はありません)。

その一方でBtrfsは、あるデータを更新する際に、元とは異なる場所に新たなデータ領域を確保した上でそこに書き込み、データへのリンクを張りなおします。

![copy_on_write](images/copy_on_write.png)

この方法であれば、いつの時点で強制電源断が発生しても、ユーザに見えるのは更新前にせよ更新後にせよ、整合性のとれた状態だけです。

# 機能説明

## ストレージプール

従来型のファイルシステムは1つのデバイスに対して1つのファイルシステムを作成します。Btrfsは、1つないし複数のデバイスから大きなストレージプールを作った上で、その上に、マウント可能なサブボリュームという領域(後述)を作成します。ストレージプールはLVMにおけるボリュームグループ、サブボリュームはLVMにおける論理ボリュームとファイルシステムを足したものに近いです。このように、Btrfsは従来型のファイルシステムの一種と考えるよりも、ファイルシステム+LVMのようなボリュームマネージャと考えるほうがわかりやすいです。

![ストレージプール](images/storage_pool.png)


既存のBtrfsファイルシステムに対するデバイスの追加、削除、交換も可能です。それに伴う容量変化に伴うファイルシステムのリサイズ処理は不要です。


![追加、削除、交換](images/add_del_replace.png)


運用中に各デバイス内のデータ使用量が偏らないようにするための、デバイス間のデータ量を平準化させることもできます

![バランス](images/balance.png)

## サブボリューム

Btrfsのファイルシステムは、作成時にサブボリュームと呼ばれるマウント可能な領域を1つ持っています。サブボリュームは追加で任意の個数を作れます。各サブボリュームは、ストレージプール内の容量を共有しているため、LVMの論理ボリュームのように、個々のボリュームごとに容量を決める必要はありません。LVMとBtrfsにおいて、それぞれ使用量が同じ論理ボリューム、あるいはサブボリュームを作った場合の例を示します。

![サブボリューム](images/subvol.png)

LVMの論理ボリュームはそれぞれ独自の容量を持っているため、空き領域もそれぞれ別に存在します。論理ボリュームの空き領域を増やしたい場合は、ひとつづつ論理ボリュームの拡張、およびファイルシステムの拡張をする必要があります。それに対してBtrfsは全てのサブボリューム間でストレージプール内の容量を共有しているため、気にすべき空き領域はストレージプールのものだけです。容量を追加したければストレージプールにデバイスを追加すればいいだけです。

## スナップショット



Btrfsは、サブボリューム単位でスナップショットを採取できます。スナップショットの作成はデータのフルコピーではなく、データを参照するメタデータの作成、およびスナップショット内のダーティページのライトバックだけで済むため、通常のコピー操作よりもはるかに高速です。元のサブボリュームとスナップショットはデータを共有するため、空間的コストも低いです。

通常のコピーとBtrfsにおけるサブボリュームのスナップショット採取との空間的コストの差を見てみましょう。まずは通常のコピーです。

![コピー](images/copy.png)

この場合、メタデータを新規作成した上で、データをすべてコピーしていることがわかります。

続いてスナップショットを見てみましょう。

![スナップショット](images/snap.png)

この場合、ツリーの根ノードだけ新規作成して、そこから次のレベルのノードへのリンクを張るだけで済みます。

スナップショット作成後に、元のスナップショットあるいはスナップショットのデータを更新すると、コピーオンライト形式で変更した箇所のデータ共有が解除されて、ストレージの使用量が増えます。

![スナップショットの更新](images/snap_update.png)

作成したスナップショットはサブボリュームと同様、マウント可能です。読み出し専用スナップショットも作れます。

スナップショットの回数には事実上無制限であり、かつ、任意の回数の多段スナップショットも採取可能です。


Btrfsは、サブボリューム単位だけではなく、ファイル単位でもスナップショット相当のものを採取できます。この機能はreflinkと呼ばれています。reflinkもスナップショット同様、新たなファイル用のメタデータのみを作成して、データは元のファイルと共有します。どちらかに書き込みをするとCoW形式でデータの共有が解除されて、ストレージ使用量が増えます。


## サブボリュームの転送



サブボリュームの内容をシリアライズしてストリーム転送できます。ファイルシステム内部構造を意識して動作するため、高速であり、かつ、転送データ量も少ないです。転送元サブボリュームは読み出し専用でなければなりません。転送したデータはローカルファイルシステム上のファイル(Btrfs以外のファイルシステム上でも可)に保存してもよいですし、ネットワーク越しに別のホストに転送してもよいです。シリアライズしたデータは転送元と同じ、あるいは異なるBtrfsファイルシステム上でサブボリュームとして復元できます。

次に示す図は、サブボリュームAをシリアライズしたデータを

- ファイルに保存、あるいは
- ネットワーク越しに別ホストに転送、あるいは
- サブボリュームBの配下に復元

した図です。

![send](images/send.png)

1つのサブボリュームだけではなく、それぞれデータを共有する2つのサブボリューム間の差分を転送することもできます。典型的には親子関係を持つ多段スナップショットに対して使います。親スナップショットを別の場所にストリーム転送したのち、親子スナップショットの差分を転送します。転送先ではまず親スナップショットを復元して、その後、このスナップショットと上記の差分データを元に、子スナップショットも復元できます。後述の通り、これは差分バックアップに利用できます。次に示す図は、互いにデータを共有するサブボリュームA,Bについて、まずはAを、続いてAとBの差分をリモートホストのBtrfsファイルシステム上で復元する様子です。

![send_incremental](images/send_incremental.png)

## RAID



BtrfsではRAID構成を組めます。サポートしているのはRAID0,1,10,5,6、それにdup(同じデータを同じデバイスに二重化。シングルデバイス用)です。このうち、本書執筆時点ではRAID5,6はまだ安定していないため、一般ユーザが使うものではなく、開発者がテスト用に使うものという扱いです。どのRAIDレベルにするかという設定の単位はサブボリュームごとではなく、Btrfsファイルシステム全体です。

RAID無しの構成の例を見てみましょう。次の図では、sdaとsdbの上に作成したsingle構成のBtrfsファイルシステムにサブボリュームAが存在しています。このときsdaが壊れるとサブボリュームAのデータは全て失ってしまいます。

![raid_single](images/raid_single.png)

これに対して同ファイルシステムをRAID1構成にしていれば、すべてのデータは2つのストレージ(この場合はsdaとsdb)に書かれるため、sdaが壊れてもAのデータはsdb上に残っています。

![raid_1](images/raid_1.png)

要件に応じて、ユーザのデータそのものと、データの格納場所やファイルの属性などのを保持するメタデータに対して、別のRAID構成をすることもできます。後からRAID構成を変更することもできます。



## データ破壊検知/修復


Btrfsにおいてはストレージ内の一部のデータが破壊した場合にそれを検知し、所定のRAID構成なら修復もします。このような機能を持たないファイルシステムでは次の図のように、書き込み時のビット化けなどの様々な理由によってストレージ内のデータが破壊されても、それを検出できないために、そのまま運用継続してしまいます。

![parity_none](images/parity_none.png)

これをきっかけにさらなるデータ破壊が起きることもありますし、このような障害は原因究明が困難でもあります。

その一方でBtrfsはデータ、メタデータ共に、所定のデータ長ごとにチェックサムを持つことによってデータの破壊検知が可能です。次の図のように、データ(あるいはメタデータ)を読み出す際にチェックサムエラーを検出すると、そのデータは捨てて、読み出しを依頼したユーザプログラムにはI/Oエラーが通知されます。

![parity_raid_single](images/parity_raid_single.png)

このときRAID1,10,5,6,dup構成であれば、もう一方のチェックサムが一致する正しいデータをもとに、破壊されたデータを修復します。RAID5,6の場合もパリティを使って同様のことをします。次の図はRAID1構成の場合の流れです。

![parity_raid_1](images/parity_raid_1.png)


この場合、読み出し元はデータが一時的に壊れていたことを意識せずに、処理を継続できます。

本書執筆時点では、メタデータにチェックサムを付けることによって壊れたメタデータを検出、破棄できるファイルシステムは他にも存在しますが、データもメタデータも検出、破棄、修復可能なファイルシステムはBtrfsのみです。

ユーザにより読み出された個々のデータの破壊検知/修復だけではなく、scrubと呼ばれるファイルシステム全体の走査によって、すべてのデータについての破壊検知/修復も可能です。scrubは運用を止めることなく、オンラインで実行可能です。

## 透過的圧縮


ユーザデータを圧縮してストレージ使用量、およびI/O帯域を減らすこともできます。ユーザデータの書き込み時に、データを一旦圧縮した上でストレージに書き込みます。その後に当該データが読み出される際は、今度は圧縮されたデータを展開して元のデータに復元します。

次の図は通常のI/Oの様子を表しています。

![transparent_compression](images/transparent_compression_off.png)

次の図は透過的圧縮を有効にした場合のI/Oの様子です。

![transparent_compression](images/transparent_compression_on.png)

上の図よりも下の図のほうがI/O量、ストレージ使用量ともに少なくなっていることがわかります。


この機能を使うとデータの圧縮、展開に追加のCPUリソースが必要になるため、使用前には十分な評価が必要です。CPUリソースが有り余っているようなシステムではこの機能は有用である一方、非力なCPUを搭載したシステムにおいては、システムの運用に必要なCPUリソースが足りなくなるという問題が発生する恐れがあります。
